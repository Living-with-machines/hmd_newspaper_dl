{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hmd_newspaper_dl\n",
    "> Download Heritage made Digital Newspaper from the BL repository "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this code is to make it easier to download all of the [Heritage Made Digital Newspapers](https://bl.iro.bl.uk/collections/353c908d-b495-4413-b047-87236d2573e3?locale=en) from the British Library's [Research Repository](bl.iro.bl.uk/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import concurrent\n",
    "import itertools\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import random\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from operator import itemgetter\n",
    "\n",
    "# from os import umask\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fastcore.script import *\n",
    "from fastcore.test import *\n",
    "from fastcore.net import urlvalid\n",
    "from loguru import logger\n",
    "from nbdev.showdoc import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting newspaper links\n",
    " \n",
    "The Newspapers are currently organised by newspaper title under a collection:\n",
    "\n",
    "![](docs/images/repo_overview.png)\n",
    "\n",
    "Under each titles you can download a zip file representing a year for that particular newspaper title \n",
    "\n",
    "![](docs/images/title_overview.png)\n",
    "\n",
    "If we only want a subset of year or titles we could download these manually but if we're interested in using computational methods it's a bit slow. What we need to do is grab all of the URL's for each title so we can bulk download them all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_link(x: str):\n",
    "    end = x.split(\"/\")[-1]\n",
    "    return \"https://bl.iro.bl.uk/concern/datasets/\" + end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a smaller helper function that will generate the correct url once we have got an ID for a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@lru_cache(256)\n",
    "def get_newspaper_links():\n",
    "    \"\"\"Returns titles from the Newspaper Collection\"\"\"\n",
    "    urls = [\n",
    "        f\"https://bl.iro.bl.uk/collections/9a6a4cdd-2bfe-47bb-8c14-c0a5d100501f?locale=en&page={page}\"\n",
    "        for page in range(1, 3)\n",
    "    ]\n",
    "    link_tuples = []\n",
    "    for url in urls:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        links = soup.select(\".hyc-container > .hyc-bl-results a[id*=src_copy_link]\")\n",
    "        \n",
    "        for link in links:\n",
    "            url = link[\"href\"]\n",
    "            if url:\n",
    "                t = (link.text, _get_link(url))\n",
    "                link_tuples.append(t)\n",
    "        return link_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function starts from the Newspaper collection and then uses BeatifulSoup to scrape all of the URLs which link to a newspaper title. We have a hard coded URL here which isn't very good practice but since we're writing this code for a fairly narrow purpose we'll not worry about that here. \n",
    "\n",
    "If we call this function we get a bunch of links back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('The Express',\n  'https://bl.iro.bl.uk/concern/datasets/93ec8ab4-3348-409c-bf6d-a9537156f654?locale=en'),\n ('The Press.',\n  'https://bl.iro.bl.uk/concern/datasets/2f70fbcd-9530-496a-903f-dfa4e7b20d3b?locale=en'),\n ('The Star',\n  'https://bl.iro.bl.uk/concern/datasets/dd9873cf-cba1-4160-b1f9-ccdab8eb6312?locale=en'),\n ('National Register.',\n  'https://bl.iro.bl.uk/concern/datasets/f3ecea7f-7efa-4191-94ab-e4523384c182?locale=en'),\n ('The Statesman',\n  'https://bl.iro.bl.uk/concern/datasets/551cdd7b-580d-472d-8efb-b7f05cf64a11?locale=en'),\n ('The British Press; or, Morning Literary Advertiser',\n  'https://bl.iro.bl.uk/concern/datasets/aef16a3c-53b6-4203-ac08-d102cb54f8fa?locale=en'),\n ('The Sun',\n  'https://bl.iro.bl.uk/concern/datasets/b9a877b8-db7a-4e5f-afe6-28dc7d3ec988?locale=en'),\n ('The Liverpool Standard etc',\n  'https://bl.iro.bl.uk/concern/datasets/fb5e24e3-0ac9-4180-a1f4-268fc7d019c1?locale=en'),\n ('Colored News',\n  'https://bl.iro.bl.uk/concern/datasets/bacd53d6-86b7-4f8a-af31-0a12e8eaf6ee?locale=en'),\n ('The Northern Daily Times etc',\n  'https://bl.iro.bl.uk/concern/datasets/5243dccc-3fad-4a9e-a2c1-d07e750c46a6?locale=en')]"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = get_newspaper_links()\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is code has fairly narrow scope, we might still want some tests to check we're not completely off. `nbdev` makes this super easy. Here we get that the we get back what we expect in terms of tuple length and that our urls look like urls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(links[0]) == 2  # test tuple len\n",
    "assert (\n",
    "    next(iter(set(map(urlvalid, map(itemgetter(1), links))))) == True\n",
    ")  # check second item valid url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(links) == 10\n",
    "assert type(links[0]) == tuple\n",
    "assert (list(map(itemgetter(1), links))[-1]).startswith(\"https://\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@lru_cache(256)\n",
    "def get_download_urls(url: str) -> list:\n",
    "    \"\"\"Given a dataset page on the IRO repo return all download links for that page\"\"\"\n",
    "    data, urls = None, None\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "    except requests.exceptions.MissingSchema as E:\n",
    "        print(E)\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    link_ends = soup.find_all(\"a\", id=\"file_download\")\n",
    "    urls = [\"https://bl.iro.bl.uk\" + link[\"href\"] for link in link_ends]\n",
    "    # data = json.loads(soup.find(\"script\", type=\"application/ld+json\").string)\n",
    "    # except AttributeError as E:\n",
    "    #     print(E)\n",
    "    # if data:\n",
    "    #     #data = data[\"distribution\"]\n",
    "    #     #urls = [item[\"contentUrl\"] for item in data]\n",
    "    return list(set(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_download_urls` takes a 'title' URL and then grabs all of the URLs for the zip files related to that title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'https://bl.iro.bl.uk/concern/datasets/93ec8ab4-3348-409c-bf6d-a9537156f654?locale=en'"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_link = links[0][1]\n",
    "test_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['https://bl.iro.bl.uk/downloads/9c4f2fd6-d58c-4a57-8fac-a5dd273f8ed3?locale=en',\n 'https://bl.iro.bl.uk/downloads/5b450972-990c-4ed5-a979-2c3fef6d0c4a?locale=en',\n 'https://bl.iro.bl.uk/downloads/e89ca9c4-b101-44bf-b1de-15052eb63d5e?locale=en',\n 'https://bl.iro.bl.uk/downloads/319d5656-94b0-4cbf-8f0d-d3ce0aa3ab40?locale=en',\n 'https://bl.iro.bl.uk/downloads/80708825-d96a-4301-9496-9598932520f4?locale=en',\n 'https://bl.iro.bl.uk/downloads/ebd5d9eb-e0ec-40b0-ae10-132cdfbaa4e1?locale=en',\n 'https://bl.iro.bl.uk/downloads/7c2cf32f-5767-4632-87d0-3001fc5689cc?locale=en',\n 'https://bl.iro.bl.uk/downloads/50ebdb11-9186-4c24-90e5-27caf73d3f11?locale=en',\n 'https://bl.iro.bl.uk/downloads/3fd6b687-feb0-4d92-b8d7-4ea0acc5346c?locale=en',\n 'https://bl.iro.bl.uk/downloads/b40aabab-b366-4148-975e-4481d30ba182?locale=en',\n 'https://bl.iro.bl.uk/downloads/9c24784d-56e6-44c1-bcc6-774fadc87718?locale=en',\n 'https://bl.iro.bl.uk/downloads/050096c0-0166-4af4-89d7-29143ce8c73c?locale=en',\n 'https://bl.iro.bl.uk/downloads/5072df1a-75f3-4379-961a-59ac3566bc2f?locale=en',\n 'https://bl.iro.bl.uk/downloads/54d974ba-fcb2-4566-a5ac-b66d85954963?locale=en',\n 'https://bl.iro.bl.uk/downloads/0ea7aa1f-3b4f-4972-bc12-b7559769471f?locale=en',\n 'https://bl.iro.bl.uk/downloads/2997c3ff-323f-45e6-ac1c-4a147b7c78ff?locale=en',\n 'https://bl.iro.bl.uk/downloads/0fd85a65-bfa3-4db8-8b92-7fc305cab4d4?locale=en',\n 'https://bl.iro.bl.uk/downloads/30b3e2ac-2e49-410d-8635-dfa69b23f65c?locale=en',\n 'https://bl.iro.bl.uk/downloads/e272c936-24ac-4702-bdee-483ec9b0c8be?locale=en',\n 'https://bl.iro.bl.uk/downloads/17b6e110-8ed0-46cb-8030-6cc7f387ade5?locale=en',\n 'https://bl.iro.bl.uk/downloads/a7a674bf-2517-4fbc-ad20-14d61646d80e?locale=en',\n 'https://bl.iro.bl.uk/downloads/aa8b9145-a7d9-4869-8f3e-07d864238ff0?locale=en',\n 'https://bl.iro.bl.uk/downloads/7ac7a0cb-29a2-4172-8b79-4952e2c9b128?locale=en']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_download_urls(test_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_session() -> requests.sessions.Session:\n",
    "    \"\"\"returns a requests session\"\"\"\n",
    "    retry_strategy = Retry(total=60)\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session = requests.Session()\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_session` just adds some extra things to our `Requests` session to try and make it a little more robust. This is probably not necessary here but it can be useful to bump up the number of retries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _download(url: str, dir: Union[str, Path]=None, return_filename_only: bool=False, sleepy=True):\n",
    "    if sleepy: # This is to avoid hammering the BL repository with too many requests\n",
    "        time.sleep(10)\n",
    "    fname = None\n",
    "    s = create_session()\n",
    "    try:\n",
    "        r = s.get(url, stream=True, timeout=(30))\n",
    "        r.raise_for_status()\n",
    "        # fname = r.headers[\"Content-Disposition\"].split('_')[1]\n",
    "        fname = \"_\".join(r.headers[\"Content-Disposition\"].split('\"')[1].split(\"_\")[0:5])\n",
    "        if fname and return_filename_only:\n",
    "            return fname\n",
    "        if fname:\n",
    "            with open(f\"{dir}/{fname}\", \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except requests.exceptions.RequestException as request_exception:\n",
    "        logger.error(request_exception)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url in get_download_urls(\"https://bl.iro.bl.uk/concern/datasets/93ec8ab4-3348-409c-bf6d-a9537156f654\"):\n",
    "#     s = create_session()\n",
    "#     r = s.get(url, stream=True, timeout=(30))\n",
    "#     print(\"_\".join(r.headers[\"Content-Disposition\"].split('\"')[1].split(\"_\")[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = create_session()\n",
    "# r = s.get(test_url, stream=True, timeout=(30))\n",
    "# \"_\".join(r.headers[\"Content-Disposition\"].split('\"')[1].split(\"_\")[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This downloads a file and logs an exception if something goes wrong. Again we do a little test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'BLNewspapers_0002642_TheExpress_1848_f1c4cb8d-6bd5-401f-831f-a19199d47c0a.zip'"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "test_url = (\n",
    "    \"https://bl.iro.bl.uk/downloads/0ea7aa1f-3b4f-4972-bc12-b7559769471f?locale=en\"\n",
    ")\n",
    "Path(\"test_dir\").mkdir()\n",
    "test_dir = Path(\"test_dir\")\n",
    "_download(test_url, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "assert list(test_dir.iterdir())[0].suffix == \".zip\"\n",
    "assert len(list(test_dir.iterdir())) == 1\n",
    "# tidy up\n",
    "[f.unlink() for f in test_dir.iterdir()]\n",
    "test_dir.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 11:32:28.192 | ERROR    | __main__:_download:20 - HTTPSConnectionPool(host='bl.oar.bl.uk', port=443): Max retries exceeded with url: /fail_uploads/download_file?fileset_id=0ea7aa1-3b4f-4972-bc12-b75597694f (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1091)')))\n"
     ]
    }
   ],
   "source": [
    "# basic test to check bad urls won't raise unhandled exceptions\n",
    "bad_link = \"https://bl.oar.bl.uk/fail_uploads/download_file?fileset_id=0ea7aa1-3b4f-4972-bc12-b75597694f\"\n",
    "_download(bad_link, \"test_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_links = get_download_urls(test_link)\n",
    "url = download_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = _download(url,None, return_filename_only=True)\n",
    "assert fname\n",
    "assert isinstance(fname, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_from_urls(urls: List[str], save_dir: Union[str, Path], n_threads: int = 4):\n",
    "    \"\"\"Downloads from an input lists of `urls` and saves to `save_dir`, option to set `n_threads` default = 8\"\"\"\n",
    "    download_count = 0\n",
    "    tic = time.perf_counter()\n",
    "    Path(save_dir).mkdir(exist_ok=True)\n",
    "    logger.remove()\n",
    "    logger.add(lambda msg: tqdm.write(msg, end=\"\"))\n",
    "    with tqdm(total=len(urls)) as progress:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "            future_to_url = {\n",
    "                executor.submit(_download, url, save_dir): url for url in urls\n",
    "            }\n",
    "            for future in future_to_url:\n",
    "                future.add_done_callback(lambda p: progress.update(1))\n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                except Exception as e:\n",
    "                    logger.error(\"%r generated an exception: %s\" % (url, e))\n",
    "                else:\n",
    "                    if data:\n",
    "                        logger.info(f\"{url} downloaded to {data}\")\n",
    "                        download_count += 1\n",
    "        toc = time.perf_counter()\n",
    "    logger.remove()\n",
    "    logger.info(f\"Downloads completed in {toc - tic:0.4f} seconds\")\n",
    "    return download_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`download_from_urls` takes a list of urls and downloads it to a specified directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_links = [\n",
    "    \"https://bl.iro.bl.uk/downloads/0ea7aa1f-3b4f-4972-bc12-b7559769471f?locale=en\",\n",
    "    \"https://bl.iro.bl.uk/downloads/80708825-d96a-4301-9496-9598932520f4?locale=en\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [01:04<01:04, 64.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-16 11:33:43.085 | INFO     | __main__:download_from_urls:24 - https://bl.iro.bl.uk/downloads/0ea7aa1f-3b4f-4972-bc12-b7559769471f?locale=en downloaded to BLNewspapers_0002642_TheExpress_1848_f1c4cb8d-6bd5-401f-831f-a19199d47c0a.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:11<00:00, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-16 11:33:49.963 | INFO     | __main__:download_from_urls:24 - https://bl.iro.bl.uk/downloads/80708825-d96a-4301-9496-9598932520f4?locale=en downloaded to BLNewspapers_0002642_TheExpress_1847_8f13ba53-0e13-4409-a384-830ba2b160db.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_from_urls(test_links, \"test_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "assert len(test_links) == len(os.listdir(\"test_dir\"))\n",
    "test_dir = Path(\"test_dir\")\n",
    "[f.unlink() for f in test_dir.iterdir()]\n",
    "test_dir.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-16 11:34:18.149 | ERROR    | __main__:_download:20 - HTTPSConnectionPool(host='bl.oar.bl.uk', port=443): Max retries exceeded with url: /fail_uploads/download_file?fileset_id=0ea7aa1f-3b4f-4972-bc12-b7559769471f (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1091)')))\n",
      "2022-06-16 11:34:18.150 | ERROR    | __main__:_download:20 - HTTPSConnectionPool(host='bl.oar.bl.uk', port=443): Max retries exceeded with url: /fail_uploads/download_file?fileset_id=7ac7a0cb-29a2-4172-8b79-4952e2c9b (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1091)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "test_some_bad_links = [\n",
    "    \"https://bl.oar.bl.uk/fail_uploads/download_file?fileset_id=0ea7aa1f-3b4f-4972-bc12-b7559769471f\",\n",
    "    \"https://bl.oar.bl.uk/fail_uploads/download_file?fileset_id=7ac7a0cb-29a2-4172-8b79-4952e2c9b\",\n",
    "]\n",
    "download_from_urls(test_some_bad_links, \"test_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "test_dir = Path(\"test_dir\")\n",
    "[f.unlink() for f in test_dir.iterdir()]\n",
    "test_dir.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@call_parse\n",
    "def cli(\n",
    "    save_dir: Param(\"Output Directory\", str),\n",
    "    n_threads: Param(\"Number threads to use\") = 8,\n",
    "    subset: Param(\"Download subset of HMD\", int, opt=True) = None,\n",
    "    url: Param(\"Download from a specific URL\", str, opt=True) = None,\n",
    "):\n",
    "    \"Download HMD newspaper from iro to `save_dir` using `n_threads`\"\n",
    "    if url is not None:\n",
    "        logger.info(f\"Getting zip download file urls for {url}\")\n",
    "        try:\n",
    "            zip_urls = get_download_urls(url)\n",
    "            print(zip_urls)\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "        download_count = download_from_urls(zip_urls, save_dir, n_threads=n_threads)\n",
    "    else:\n",
    "        logger.info(\"Getting title urls\")\n",
    "        title_urls = get_newspaper_links()\n",
    "        logger.info(f\"Found {len(title_urls)} title urls\")\n",
    "        all_urls = []\n",
    "        print(title_urls)\n",
    "        for url in title_urls:\n",
    "            logger.info(f\"Getting zip download file urls for {url}\")\n",
    "            try:\n",
    "                zip_urls = get_download_urls(url[1])\n",
    "                all_urls.append(zip_urls)\n",
    "            except Exception as e:\n",
    "                logger.error(e)\n",
    "        all_urls = list(itertools.chain(*all_urls))\n",
    "        if subset:\n",
    "            if len(all_urls) < subset:\n",
    "                raise ValueError(\n",
    "                    f\"Size of requested sample {subset} is larger than total number of urls:{all_urls}\"\n",
    "                )\n",
    "            all_urls = random.sample(all_urls, subset)\n",
    "        print(all_urls)\n",
    "        download_count = download_from_urls(all_urls, save_dir, n_threads=n_threads)\n",
    "        request_url_count = len(all_urls)\n",
    "        if request_url_count == download_count:\n",
    "            logger.info(\n",
    "                f\"\\U0001F600 Requested count of urls: {request_url_count} matches number downloaded: {download_count}\"\n",
    "            )\n",
    "        if request_url_count > download_count:\n",
    "            logger.warning(\n",
    "                f\"\\U0001F622 Requested count of urls: {request_url_count} higher than number downloaded: {download_count}\"\n",
    "            )\n",
    "        if request_url_count < download_count:\n",
    "            logger.warning(\n",
    "                f\"\\U0001F937 Requested count of urls: {request_url_count} lower than number downloaded: {download_count}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally use `fastcore` to make a little CLI that we can use to download all of our files. We even get a little help flag for free ðŸ˜€. We can either call this as a python function, or when we install the python package it gets registered as a `console_scripts` and can be used like other command line tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cli(\"test_dir\", subset=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert all([f.suffix == '.zip' for f in Path(\"test_dir\").iterdir()])\n",
    "# assert len(list(Path(\"test_dir\").iterdir())) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dir = Path(\"test_dir\")\n",
    "# [f.unlink() for f in test_dir.iterdir()]\n",
    "# test_dir.rmdir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
